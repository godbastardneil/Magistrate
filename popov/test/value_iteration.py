# Инициализировать модель Марковского процесса принятия решений
actions = (0, 1)  # действия (0=left, 1=right)
states = (0, 1, 2, 3, 4)  # состояния (плитка)
rewards = [-1, -1, 10, -1, -1]  # Прямые награды за состояние
gamma = 0.9  # коэффициент дисконтирования
# Вероятности перехода на пару состояние-действие
probs = [
    [[0.9, 0.1], [0.1, 0.9], [0, 0], [0, 0], [0, 0]],
    [[0.9, 0.1], [0, 0], [0.1, 0.9], [0, 0], [0, 0]],
    [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0]],  # Terminating state (все пути 0)
    [[0, 0], [0, 0], [0.9, 0.1], [0, 0], [0.1, 0.9]],
    [[0, 0], [0, 0], [0, 0], [0.9, 0.1], [0.1, 0.9]],
]

# Установить значение параметров итерации
max_iter = 10000  # Максимальное количество итераций
delta = 1e-400  # Допуск к ошибкам
V = [0, 0, 0, 0, 0]  # Инициализировать значения
pi = [None, None, None, None, None]  # Инициализировать политику


# Начать итерацию значений
for i in range(max_iter):
    max_diff = 0  # Инициализировать максимальную разницу
    V_new = [0, 0, 0, 0, 0]  # Инициализировать значения
    for s in states:
        max_val = 0
        for a in actions:

            # Вычислить значение состояния
            val = rewards[s]  # Получите прямое вознаграждение
            for s_next in states:
                val += probs[s][s_next][a] * (
                    gamma * V[s_next]
                )  # Добавьте нижестоящие значения со скидкой

            # Лучшее действие по сохранению стоимости на данный момент
            max_val = max(max_val, val)

            # Обновить лучшую политику
            if V[s] < val:
                pi[s] = actions[a]  # Сохранить действие с наивысшим значением

        V_new[s] = max_val  # Обновить значение с наибольшим значением

        # Обновить максимальную разницу
        max_diff = max(max_diff, abs(V[s] - V_new[s]))

    # Обновить функцию полезности
    V = V_new

    # Если diff меньше пороговой дельты для всех состояний, алгоритм завершает работу.
    if max_diff < delta:
        break
print(pi, V)